download(url, destfile=filename)
dat <- read.csv(filename)
library(downloader) ##use install.packages to install
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extd\
ata/femaleMiceWeights.csv"
filename <- "femaleMiceWeights.csv"
download(url, destfile=filename)
dat <- read.csv(filename)
head(dat)
library(dplyr)
control <- filter(dat, Diet="chow") %>% select(Bodyweight) %>% unlist
library(dplyr)
control <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
treatment <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist
print(mean(treatment))
print(mean(control))
obsdiff <- mean(treatment) - mean(control)
print(obsdiff)
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extd\
ata/femaleControlsPopulation.csv"
filename <- "femaleControlsPopulation.csv"
if (!file.exists(filename)) download(url,destfile=filename)
population <- read.csv(filename)
population <- unlist(population) # turn it into a numeric
control <- sample(population,12)
mean(control)
control <- sample(population,12)
mean(control)
control <- sample(population,12)
mean(control)
control <- sample(population,12)
mean(control)
control <- sample(population,12)
mean(control)
# difference of averages when null hypothesis true
##12 control mice
control <- sample(population,12)
##another 12 control mice that we act as if they were not
treatment <- sample(population,12)
print(mean(treatment) - mean(control))
n <- 10000
null <- vector("numeric",n)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
null[i] <- mean(treatment) - mean(control)
}
print(null)
print(max(null))
print(min(null))
mean(null >= obsdiff)
library(UsingR)
x <- father.son$fheight
install.packages("UsingR")
library(UsingR)
x <- father.son$fheight
round(sample(x,10),1)
smallest <- floor( min(x) )
largest <- ceiling( max(x) )
values <- seq(smallest, largest,len=300)
heightecdf <- ecdf(x)
plot(values, heightecdf(values), type="l",
xlab="a (Height in inches)",ylab="Pr(x <= a)")
hist(x)
bins <- seq(smallest, largest)
hist(x,breaks=bins,xlab="Height (in inches)",main="Adult men heights")
n <- 100
library(rafalib)
nullplot(-5,5,1,30, xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
nulldiff <- mean(treatment) - mean(control)
j <- pmax(pmin(round(nulldiff)+6,11),1)
totals[j] <- totals[j]+1
text(j-6,totals[j],pch=15,round(nulldiff,1))
##if(i < 15) Sys.sleep(1) ##You can add this line to see values appear slowly
}
install.packages("rafalib")
n <- 100
library(rafalib)
nullplot(-5,5,1,30, xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
nulldiff <- mean(treatment) - mean(control)
j <- pmax(pmin(round(nulldiff)+6,11),1)
totals[j] <- totals[j]+1
text(j-6,totals[j],pch=15,round(nulldiff,1))
##if(i < 15) Sys.sleep(1) ##You can add this line to see values appear slowly
}
library(rafalib)
nullplot(-5,5,1,30, xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
nulldiff <- mean(treatment) - mean(control)
j <- pmax(pmin(round(nulldiff)+6,11),1)
totals[j] <- totals[j]+1
text(j-6,totals[j],pch=15,round(nulldiff,1))
#if(i < 15) Sys.sleep(1) ##You can add this line to see values appear slowly
}
library(rafalib)
nullplot(-5,5,1,30, xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
nulldiff <- mean(treatment) - mean(control)
j <- pmax(pmin(round(nulldiff)+6,11),1)
totals[j] <- totals[j]+1
text(j-6,totals[j],pch=15,round(nulldiff,1))
if(i < 15) Sys.sleep(1) ##You can add this line to see values appear slowly
}
library(rafalib)
nullplot(-5,5,1,30, xlab="Observed differences (grams)", ylab="Frequency")
totals <- vector("numeric",11)
for (i in 1:n) {
control <- sample(population,12)
treatment <- sample(population,12)
nulldiff <- mean(treatment) - mean(control)
j <- pmax(pmin(round(nulldiff)+6,11),1)
totals[j] <- totals[j]+1
text(j-6,totals[j],pch=15,round(nulldiff,1))
if(i < 15) Sys.sleep(0.25) ##You can add this line to see values appear slowly
}
hist(null, freq=TRUE)
abline(v=obsdiff, col="red", lwd=2)
smallest <- floor( min(x) )
pnorm(obsdiff,mean(null),sd(null))
- pnorm(obsdiff,mean(null),sd(null))
1 - pnorm(obsdiff,mean(null),sd(null))
set.seed(1)
install.packages("reticulate")
Sys.which("python")
a <- c(3,4,5,7,8)
b <- c(7,8,8,5,3)
a_df <- as.data.frame(a)
b_df <- as.data.frame(b)
shared <- a_df %in% b_df
shared
shared <- a_df$a %in% b_df$b
shared
shared <- a %in% b
shared
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 0       #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# Check if required packages are already installed, and install if missing
packages <-c("caret", "ROCR", "dplyr", "xgboost", "pROC")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# scale pre-preprocessed training data and merge phenotype column from metadata
otu_table_scaled <- scale(otu, center = TRUE, scale = TRUE)
otu_table_scaled_labels <- data.frame(t(otu_table_scaled))
otu_table_scaled_labels[col_name] <- mapping[rownames(otu_table_scaled_labels), col_name]
# convert category to continous variable
#cols_factor <- as.factor(otu_table_scaled_labels[[col_name]])
#levels(cols_factor) <- 1:length(levels(cols_factor))
#cols <- as.numeric(cols_factor)
#cols <- as.factor(cols)
#print(cols)
#otu_table_scaled_labels[[col_name]] <- cols
# set random seed to 42
set.seed(42)
#split into training and test
trainIndex = createDataPartition(otu_table_scaled_labels[[col_name]],
p=0.7, list=FALSE,times=1)
training = otu_table_scaled_labels[trainIndex,]
test = otu_table_scaled_labels[-trainIndex,]
# set X and y
X_train <- training[,1:(ncol(training)-1)]
y_train <- training[ , ncol(training)]
X_test <- test[,1:(ncol(test)-1)]
# for comparing model predictions against actual categories
actual <- as.character(test[ , ncol(test)])
actual <- as.numeric(actual)
# cross validation method
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
mtryStart <- floor(sqrt(ncol(X_train)))
mtryexpand <- seq(from = mtryStart-10, to= mtryStart+10, by=2)
tunegrid <- expand.grid(.mtry=mtryexpand)
RF_cv <- train(X_train, y_train, method="rf", ntree=500 ,
tuneGrid=tunegrid, trControl=fit_ctrl)
importance <- varImp(RF_cv)
predictions <- predict(RF_cv, newdata = X_test)
prob <- predict(RF_cv, newdata= X_test, type="prob")
samples <- row.names(X_test)
pred_df <- data.frame(samples, actual, predictions)
print(pred_df)
pred_df$Correct <- pred_df$actual == pred_df$predictions
result <- confusionMatrix(predictions, actual)
metrics <- data.frame(cbind(t(result$positive),t(result$byClass), t(result$overall)))
importance <- importance$importance
otu_names = cbind(OTU=row.names(importance), importance) # clean this code up and make sure it works in all cases (drop index also)
importance_sorted <- otu_names[order(-otu_names$Overall), , drop=FALSE]
pdf("feature_importance_top10.pdf", width = 10, height = 5)
importance_plot <- barplot(importance_sorted[1:10, "Overall"], names.arg=importance_sorted[1:10, "OTU"],
ylab="Variable Importance", las=2, ylim=c(0,100), col = "darkblue",
main="Feature Importance (Top 10)")
importance_plot
dev.off()
true_classes <- as.data.frame(test[ , ncol(test)])
model_predictions <- as.data.frame(pred_df$predictions)
write.table(importance, file="importance.tab", sep="\t")
write.table(pred_df, file = "random_forest_predictions.tab", sep="\t", row.names = FALSE) # output to folders
write.table(result$table, file = "confusion_matrix.tab", sep="\t", row.names = FALSE)
write.table(metrics, file="metrics.tab", sep="\t", row.names = FALSE)
if (cv == 0 | 1) {
roc_rf <- roc(actual, prob$`1`)
pdf("roc_curve.pdf")
roc_plot <- plot(roc_rf, col = "blue")
roc_plot
dev.off()
} else if (cv == 2) {
print("Sorry LOOCV support is not yet available")
}
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 0       #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# Check if required packages are already installed, and install if missing
packages <-c("caret", "ROCR", "dplyr", "xgboost", "pROC")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# scale pre-preprocessed training data and merge phenotype column from metadata
otu_table_scaled <- scale(otu, center = TRUE, scale = TRUE)
otu_table_scaled_labels <- data.frame(t(otu_table_scaled))
otu_table_scaled_labels[col_name] <- mapping[rownames(otu_table_scaled_labels), col_name]
# convert category to continous variable
#cols_factor <- as.factor(otu_table_scaled_labels[[col_name]])
#levels(cols_factor) <- 1:length(levels(cols_factor))
#cols <- as.numeric(cols_factor)
#cols <- as.factor(cols)
#print(cols)
#otu_table_scaled_labels[[col_name]] <- cols
# set random seed to 42
set.seed(42)
#split into training and test
trainIndex = createDataPartition(otu_table_scaled_labels[[col_name]],
p=0.7, list=FALSE,times=1)
training = otu_table_scaled_labels[trainIndex,]
test = otu_table_scaled_labels[-trainIndex,]
# set X and y
X_train <- training[,1:(ncol(training)-1)]
y_train <- training[ , ncol(training)]
X_test <- test[,1:(ncol(test)-1)]
# for comparing model predictions against actual categories
actual <- as.character(test[ , ncol(test)])
# cross validation method
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
mtryStart <- floor(sqrt(ncol(X_train)))
mtryexpand <- seq(from = mtryStart-10, to= mtryStart+10, by=2)
tunegrid <- expand.grid(.mtry=mtryexpand)
RF_cv <- train(X_train, y_train, method="rf", ntree=500 ,
tuneGrid=tunegrid, trControl=fit_ctrl)
importance <- varImp(RF_cv)
predictions <- predict(RF_cv, newdata = X_test)
prob <- predict(RF_cv, newdata= X_test, type="prob")
samples <- row.names(X_test)
pred_df <- data.frame(samples, actual, predictions)
print(pred_df)
pred_df$Correct <- pred_df$actual == pred_df$predictions
result <- confusionMatrix(predictions, actual)
metrics <- data.frame(cbind(t(result$positive),t(result$byClass), t(result$overall)))
importance <- importance$importance
otu_names = cbind(OTU=row.names(importance), importance) # clean this code up and make sure it works in all cases (drop index also)
importance_sorted <- otu_names[order(-otu_names$Overall), , drop=FALSE]
pdf("feature_importance_top10.pdf", width = 10, height = 5)
importance_plot <- barplot(importance_sorted[1:10, "Overall"], names.arg=importance_sorted[1:10, "OTU"],
ylab="Variable Importance", las=2, ylim=c(0,100), col = "darkblue",
main="Feature Importance (Top 10)")
importance_plot
dev.off()
true_classes <- as.data.frame(test[ , ncol(test)])
model_predictions <- as.data.frame(pred_df$predictions)
write.table(importance, file="importance.tab", sep="\t")
write.table(pred_df, file = "random_forest_predictions.tab", sep="\t", row.names = FALSE) # output to folders
write.table(result$table, file = "confusion_matrix.tab", sep="\t", row.names = FALSE)
write.table(metrics, file="metrics.tab", sep="\t", row.names = FALSE)
if (cv == 0 | 1) {
roc_rf <- roc(actual, prob$`1`)
pdf("roc_curve.pdf")
roc_plot <- plot(roc_rf, col = "blue")
roc_plot
dev.off()
} else if (cv == 2) {
print("Sorry LOOCV support is not yet available")
}
actual
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 0       #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# Check if required packages are already installed, and install if missing
packages <-c("caret", "ROCR", "dplyr", "xgboost", "pROC")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# scale pre-preprocessed training data and merge phenotype column from metadata
otu_table_scaled <- scale(otu, center = TRUE, scale = TRUE)
otu_table_scaled_labels <- data.frame(t(otu_table_scaled))
otu_table_scaled_labels[col_name] <- mapping[rownames(otu_table_scaled_labels), col_name]
# convert category to continous variable
cols_factor <- as.factor(otu_table_scaled_labels[[col_name]])
levels(cols_factor) <- 1:length(levels(cols_factor))
cols <- as.numeric(cols_factor)
cols <- as.factor(cols)
print(cols)
otu_table_scaled_labels[[col_name]] <- cols
# set random seed to 42
set.seed(42)
#split into training and test
trainIndex = createDataPartition(otu_table_scaled_labels[[col_name]],
p=0.7, list=FALSE,times=1)
training = otu_table_scaled_labels[trainIndex,]
test = otu_table_scaled_labels[-trainIndex,]
# set X and y
X_train <- training[,1:(ncol(training)-1)]
y_train <- training[ , ncol(training)]
X_test <- test[,1:(ncol(test)-1)]
# for comparing model predictions against actual categories
actual <- as.character(test[ , ncol(test)])
actual <- as.numeric(actual)
# cross validation method
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
mtryStart <- floor(sqrt(ncol(X_train)))
mtryexpand <- seq(from = mtryStart-10, to= mtryStart+10, by=2)
tunegrid <- expand.grid(.mtry=mtryexpand)
RF_cv <- train(X_train, y_train, method="rf", ntree=500 ,
tuneGrid=tunegrid, trControl=fit_ctrl)
importance <- varImp(RF_cv)
predictions <- predict(RF_cv, newdata = X_test)
prob <- predict(RF_cv, newdata= X_test, type="prob")
samples <- row.names(X_test)
pred_df <- data.frame(samples, actual, predictions)
print(pred_df)
pred_df$Correct <- pred_df$actual == pred_df$predictions
result <- confusionMatrix(predictions, actual)
metrics <- data.frame(cbind(t(result$positive),t(result$byClass), t(result$overall)))
importance <- importance$importance
otu_names = cbind(OTU=row.names(importance), importance) # clean this code up and make sure it works in all cases (drop index also)
importance_sorted <- otu_names[order(-otu_names$Overall), , drop=FALSE]
pdf("feature_importance_top10.pdf", width = 10, height = 5)
importance_plot <- barplot(importance_sorted[1:10, "Overall"], names.arg=importance_sorted[1:10, "OTU"],
ylab="Variable Importance", las=2, ylim=c(0,100), col = "darkblue",
main="Feature Importance (Top 10)")
importance_plot
dev.off()
true_classes <- as.data.frame(test[ , ncol(test)])
model_predictions <- as.data.frame(pred_df$predictions)
write.table(importance, file="importance.tab", sep="\t")
write.table(pred_df, file = "random_forest_predictions.tab", sep="\t", row.names = FALSE) # output to folders
write.table(result$table, file = "confusion_matrix.tab", sep="\t", row.names = FALSE)
write.table(metrics, file="metrics.tab", sep="\t", row.names = FALSE)
if (cv == 0 | 1) {
roc_rf <- roc(actual, prob$`1`)
pdf("roc_curve.pdf")
roc_plot <- plot(roc_rf, col = "blue")
roc_plot
dev.off()
} else if (cv == 2) {
print("Sorry LOOCV support is not yet available")
}
