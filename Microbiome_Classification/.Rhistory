)
stop_script_if_missing <- function(otu, mapping) {
stopifnot(exists(otu), exists(mapping))
}
build_model <- function(){
# leave blank right now
}
preprocess <- function(otu, scale = "clr", mapping, class_col) {
scaled_otu <- clr(otu)
otu_scaled_labels <- data.frame(t(scaled_otu))
otu_scaled_labels["Class"] <- mapping[rownames(otu_scaled_labels), class_col]
return(otu_scaled_labels)
}
check_dim <- function(df) {
if (dim(otu)[2] < 50) {
print("Warning: small sample size detected, classifications and feature importances may be less accurate/useful")
}
}
get_cv <- function(cv) {
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
return(fit_ctrl)
}
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# check dimensions of otu table and print warning message if sample size small
check_dim(otu)
stop_script_if_missing("otu", "mapping")
stopifnot(exists("otu"), exists("mapping"))
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9.0
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map-madeupmulticlass.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 3      #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# could update this to pacman or at least add if else for bioconductor
# Check if required packages are already installed, and install if missing
packages <-c("caret", "dplyr", "xgboost", "pROC", "ggplot2", "compositions")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
#################################################################################################################################################
# functions
defaults <- list(model = "Random Forest",
train_size = 0.7,
)
build_model <- function(){
# leave blank right now
}
preprocess <- function(otu, scale = "clr", mapping, class_col) {
scaled_otu <- clr(otu)
otu_scaled_labels <- data.frame(t(scaled_otu))
otu_scaled_labels["Class"] <- mapping[rownames(otu_scaled_labels), class_col]
return(otu_scaled_labels)
}
check_dim <- function(df) {
if (dim(otu)[2] < 50) {
print("Warning: small sample size detected, classifications and feature importances may be less accurate/useful")
}
}
get_cv <- function(cv) {
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
return(fit_ctrl)
}
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# check dimensions of otu table and print warning message if sample size small
check_dim(otu)
stopifnot(exists("otu"), exists("mapping"))
?createDataPartition
#split into training and test = could be function
test_train <- function(otu_scaled_labels, class_col, ) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=0.7,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
#split into training and test = could be function
test_train <- function(otu_scaled_labels, class_col, ) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=0.7,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
#split into training and test = could be function
test_train <- function(otu_scaled_labels, class_col, partition) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=partition,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
train_test_split <- function(otu_scaled_labels, class_col, partition) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=partition,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
#split into training and test
train_test <- train_test_split(otu_scaled_labels, class_col = "Class", partition = 0.7)
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9.0
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
#' Alternatively if you open this script from the working directory comment out the code below with a preceding hashtag (#)
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map-madeupmulticlass.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 3      #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# could update this to pacman or at least add if else for bioconductor
# Check if required packages are already installed, and install if missing
packages <-c("caret", "dplyr", "xgboost", "pROC", "ggplot2", "compositions")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
#################################################################################################################################################
# functions
defaults <- list(model = "Random Forest",
train_size = 0.7,
)
build_model <- function(){
# leave blank right now
}
preprocess <- function(otu, scale = "clr", mapping, class_col) {
scaled_otu <- clr(otu)
otu_scaled_labels <- data.frame(t(scaled_otu))
otu_scaled_labels["Class"] <- mapping[rownames(otu_scaled_labels), class_col]
return(otu_scaled_labels)
}
check_dim <- function(df) {
if (dim(otu)[2] < 50) {
print("Warning: small sample size detected, classifications and feature importances may be less accurate/useful")
}
}
get_cv <- function(cv) {
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
return(fit_ctrl)
}
train_test_split <- function(otu_scaled_labels, class_col, partition) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=partition,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# check dimensions of otu table and print warning message if sample size small
check_dim(otu)
stopifnot(exists("otu"), exists("mapping"))
# set random seed to 42
set.seed(42)
# scale pre-preprocessed training data and merge phenotype column from metadata - should maybe think about using different method of scaling clr?
otu_scaled_labels <- preprocess(otu, scale = "clr", mapping, col_name)
#split into training and test
train_test <- train_test_split(otu_scaled_labels, class_col = "Class", partition = 0.7)
train_test$X_train
train_test$y_train
?train
mtryStart <- floor(sqrt(ncol(train_test$X_train)))
mtryexpand <- seq(from = mtryStart-10, to= mtryStart+10, by=2)
tunegrid <- expand.grid(.mtry=mtryexpand)
rf_tune <- function(X, mtry_step) {
mtryStart <- floor(sqrt(ncol(x)))
mtryexpand <- seq(from = mtryStart-mtry_step, to= mtryStart +mtry_step, by = 2)
tunegrid <- expand.grid(.mtry=mtryexpand)
return(tunegrid)
}
#
RF_cv <- build_model(X_train, y_train, method="rf", ntree=500 ,
tuneGrid=tunegrid, trControl=fit_ctrl)
#
RF_cv <- build_model(train_test$X_train, train_test$y_train, method="rf", ntree=500 ,
tuneGrid=tunegrid, trControl=fit_ctrl)
#
RF_cv <- build_model(train_test$X_train, train_test$y_train, trControl=fit_ctrl, method="rf", ntree=500 ,
tuneGrid=tunegrid)
#
RF_cv <- train(train_test$X_train, train_test$y_train, trControl=fit_ctrl, method="rf", ntree=500 ,
tuneGrid=tunegrid)
# cross validation method  - re-do as function input cv and return fit_ctrl
fit_ctrl <- get_cv(cv)
# scale pre-preprocessed training data and merge phenotype column from metadata - should maybe think about using different method of scaling clr?
otu_scaled_labels <- preprocess(otu, scale = "clr", mapping, col_name)
#split into training and test
train_test <- train_test_split(otu_scaled_labels, class_col = "Class", partition = 0.7)
# cross validation method  - re-do as function input cv and return fit_ctrl
fit_ctrl <- get_cv(cv)
cv <- 0    #<--- CHANGE ACCORDINGLY !!!
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
# could update this to pacman or at least add if else for bioconductor
# Check if required packages are already installed, and install if missing
packages <-c("caret", "dplyr", "xgboost", "pROC", "ggplot2", "compositions")
# cross validation method  - re-do as function input cv and return fit_ctrl
fit_ctrl <- get_cv(cv)
#
RF_cv <- train(train_test$X_train, train_test$y_train, trControl=fit_ctrl, method="rf", ntree=500 ,
tuneGrid=tunegrid)
##################################################################################################################################################
# This script performs supervised classification of a normalised OTU table.
# Version: 0.9.0
# 2018-02-05  Author: Adam Sorbie
#
#  Please set the directory of the script as the working folder (e.g C:/studyname/NGS-Data/Microbiome_classification)
#' Note: the path is denoted by forward slash "/"
#' Alternatively if you open this script from the working directory comment out the code below with a preceding hashtag (#)
setwd("C:/Users/PhD/Supervised_Classification/Microbiome_Classification")  #<--- CHANGE ACCORDINGLY !!!
# Enter name of OTU table file:
input_otu_table <- "merged_otu.tab"        #<--- CHANGE ACCORDINGLY !!!
# Enter name of mapping file:
mapping_file <- "merged_map-madeupmulticlass.tab"         #<--- CHANGE ACCORDINGLY !!!
# Please select cross-validation method:
# 0 = k-fold Cross-validation (default) -
# 1 = Repeated k-fold Cross Validation -
# 2 = Leave-one-out cross validation -
cv <- 0    #<--- CHANGE ACCORDINGLY !!!
# Please give the column where the categorical variable is found
col_name <- "Phenotype"        #<--- CHANGE ACCORDINGLY !!!
#############################                           NO CHANGES REQUIRED BELOW THIS LINE                         #############################
##################################################################################################################################################
###############################################                   Main Script                      ###############################################
##################################################################################################################################################
###############################################       Load all required libraries       ##########################################################
# could update this to pacman or at least add if else for bioconductor
# Check if required packages are already installed, and install if missing
packages <-c("caret", "dplyr", "xgboost", "pROC", "ggplot2", "compositions")
# Function to check whether the package is installed
InsPack <- function(pack)
{
if ((pack %in% installed.packages()) == FALSE) {
install.packages(pack,repos = "http://cloud.r-project.org/")
}
}
# Applying the installation on the list of packages
lapply(packages, InsPack)
# Make the libraries
lib <- lapply(packages, require, character.only = TRUE)
# Check if it was possible to install all required libraries
flag <- all(as.logical(lib))
#################################################################################################################################################
# functions - places where you do things twice could possibly be re-written with applies
defaults <- list(model = "Random Forest",
train_size = 0.7,
)
preprocess <- function(otu, scale = "clr", mapping, class_col) {
scaled_otu <- clr(otu)
otu_scaled_labels <- data.frame(t(scaled_otu))
otu_scaled_labels["Class"] <- mapping[rownames(otu_scaled_labels), class_col]
return(otu_scaled_labels)
}
check_dim <- function(df) {
if (dim(otu)[2] < 50) {
print("Warning: small sample size detected, classifications and feature importances may be less accurate/useful")
}
}
get_cv <- function(cv) {
if (cv == 0) {
fit_ctrl <- trainControl(method = "cv", number = 10)
} else if (cv == 1) {
fit_ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
} else if (cv == 2) {
fit_ctrl <- trainControl(method = "LOOCV")
} else {
print("Error please enter a valid selection:
0 = k-fold cross validation
1 = repeated k-fold cross validation
2 = leave-one-out cross validation")
}
return(fit_ctrl)
}
train_test_split <- function(otu_scaled_labels, class_col, partition) {
trainIndex <- createDataPartition(otu_scaled_labels[[class_col]], p=partition,
list = F, times = 1)
training <- otu_scaled_labels[trainIndex, ]
ncol_training <- ncol(training)
test <- otu_scaled_labels[-trainIndex,]
ncol_test <- ncol(test)
actual <- as.factor(test[ , ncol_test])
# generate X and y - probably a much cleaner way to write this
X_train <- training[,1:(ncol_training-1)]
y_train <- training[ , ncol_training]
X_test <- test[, 1:(ncol_test -1)]
# return list
return_list <- list("X_train" = X_train, "y_train" = y_train, "X_test" = X_test, "Actual" = actual,
"training_full" = training, "test_full" = test)
return(return_list)
}
rf_tune <- function(X, mtry_step) {
mtryStart <- floor(sqrt(ncol(x)))
mtryexpand <- seq(from = mtryStart-mtry_step, to= mtryStart +mtry_step, by = 2)
tunegrid <- expand.grid(.mtry=mtryexpand)
return(tunegrid)
}
build_model <- function(X, y, method, ...){
model <- train(X, y, method = method, trControl = fit_ctrl, ...)
return(model)
}
return_model_output <- function(variables) {
}
if (cv == 0 | 1) {
if (length(unique(categorical_variables[[col_name]])) == 2) {
roc_rf <- roc(actual, prob$`1`)
pdf("roc_curve.pdf")
roc_plot <- plot(roc_rf, col = "blue")
roc_plot
dev.off()
}   else {
roc_rf <- multiclass.roc(actual, prob$`1`)
pdf("roc_curve.pdf")
roc_plot <- plot(roc_rf$rocs[[3]], col = "blue")
roc_plot
dev.off()
}
} else if (cv == 2) {
print("Sorry LOOCV support is not yet available")
}
##################################################################################################################################################
# read data
otu <- read.table(input_otu_table, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
mapping <- read.table(mapping_file, sep="\t", header=T, row.names=1, stringsAsFactors=TRUE,
comment.char="", check.names=FALSE)
# check dimensions of otu table and print warning message if sample size small
check_dim(otu)
stopifnot(exists("otu"), exists("mapping"))
# set random seed to 42
set.seed(42)
# scale pre-preprocessed training data and merge phenotype column from metadata - should maybe think about using different method of scaling clr?
otu_scaled_labels <- preprocess(otu, scale = "clr", mapping, col_name)
#split into training and test
train_test <- train_test_split(otu_scaled_labels, class_col = "Class", partition = 0.7)
# cross validation method  - re-do as function input cv and return fit_ctrl
fit_ctrl <- get_cv(cv)
#
RF_cv <- train(train_test$X_train, train_test$y_train, trControl=fit_ctrl, method="rf", ntree=500 ,
tuneGrid=tunegrid)
#
tunegrid <- rf_tune(train_test$X_train, mtry_step = 10)
train_test$X_train
#
tunegrid <- rf_tune(train_test$X_train, mtry_step = 10)
rf_tune <- function(X, mtry_step) {
mtryStart <- floor(sqrt(ncol(X)))
mtryexpand <- seq(from = mtryStart-mtry_step, to= mtryStart +mtry_step, by = 2)
tunegrid <- expand.grid(.mtry=mtryexpand)
return(tunegrid)
}
#
tunegrid <- rf_tune(train_test$X_train, mtry_step = 10)
RF_cv <- train(train_test$X_train, train_test$y_train, trControl=fit_ctrl, method="rf", ntree=500 ,
tuneGrid=tunegrid)
RF_cv
importance <- varImp(RF_cv)
predictions <- predict(RF_cv, newdata = X_test)
predictions <- predict(RF_cv, newdata = train_test$X_test)
predictions
pred_stats <- function(names, actual, model_predictions) {
df <- data.frame(names, actual, predictions)
df$Correct <- df$actual == df$predictions
return(df)
}
